# mLLaVA:  

A fork of the LLaVA codebase (https://github.com/haotian-liu/LLaVA/tree/main) with added support for using the Baichuan2 models as the base LLM.  This codebase is a dependency of https://github.com/amith-ananthram/see-it-from-my-perspective/tree/main. 

If you use these models, please cite:

1) (Visual Instruction Tuning)[https://proceedings.neurips.cc/paper_files/paper/2023/hash/6dcf277ea32ce3288914faf369fe6de0-Abstract-Conference.html], Liu et al., 2024
2) (See It from My Perspective: Diagnosing the Western Cultural Bias of Large Vision-Language Models in Image Understanding)[https://arxiv.org/abs/2406.11665], Ananthram et al., 2024 
